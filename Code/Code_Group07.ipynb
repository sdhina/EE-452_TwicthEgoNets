{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"1GX1hlF1dOuW"},"source":["# Network Analysis\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ubDYg-pcpuvQ"},"source":["### Import libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":896,"status":"ok","timestamp":1686304667684,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"h--a72zddOuZ"},"outputs":[],"source":["import networkx as nx\n","import pandas as pd\n","import json\n","import matplotlib.pyplot as plt\n","import random\n","import numpy as np"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_nVwawUxdOua"},"source":["## Individual infos on ego-nets"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"aZGJOruCp654"},"source":["### Load graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34483,"status":"ok","timestamp":1686304702164,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"auuZq3LfdOua","outputId":"36c67a9b-83c5-4add-8cda-6e76dcc84f3e"},"outputs":[],"source":["# Specify the path to JSON file\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","json_file_path = '/content/drive/MyDrive/Colab Notebooks/Network machine learning/Assignments/Project/Datasets/twitch_edges.json'\n","\n","# Open the JSON file and load its contents into the `data` variable\n","with open(json_file_path, 'r') as file:\n","    data = json.load(file)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":27089,"status":"ok","timestamp":1686304729242,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"RnSlmG6OdOub"},"outputs":[],"source":["ego_graphs = {}\n","for ego_node, edges in data.items():\n","    graph = nx.Graph()\n","    graph.add_edges_from(edges)\n","    ego_graphs[ego_node] = graph"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hpokBq5rqDy4"},"source":["### Load labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":366,"status":"ok","timestamp":1686304729606,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"eCp0ZGh2lOd8","outputId":"9de1b953-49f0-4b75-91f6-789c0cc7b4e1"},"outputs":[],"source":["labels = pd.read_csv(r'/content/drive/MyDrive/Colab Notebooks/Network machine learning/Assignments/Project/Datasets/twitch_target.csv')\n","print(labels)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Z0QV8ngOqNMq"},"source":["### Separate indexes based on labels"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1686304729606,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"R_hSXJ3KoIxh"},"outputs":[],"source":["index0=labels.index[labels['target'] == 0].tolist()\n","index1=labels.index[labels['target'] == 1].tolist()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":155577,"status":"ok","timestamp":1686304885180,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"upGtEMYWdOub","outputId":"ee265278-4c00-4b7d-c636-a7b005e121b9"},"outputs":[],"source":["i=0\n","for ego_node, graph in ego_graphs.items():\n","    print(f\"Ego-Network: {ego_node}\")\n","    print(f\"Number of nodes: {graph.number_of_nodes()}\")\n","    print(f\"Number of edges: {graph.number_of_edges()}\")\n","    #print(f\"Average degree: {sum(graph.degree()) / graph.number_of_nodes()}\")\n","    print()\n","    i+=1\n","print(i)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cvZMcMn8dOuc"},"source":["### Visualize graphs with different labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":479},"executionInfo":{"elapsed":734,"status":"ok","timestamp":1686304885876,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"ALfFV6uVdOuc","outputId":"7c7e9e35-67a7-4528-a57e-7de3732cf536"},"outputs":[],"source":["#select two random graphs with labels 0 and 1 respectively\n","id0 = random.choice(index0)\n","id1 = random.choice(index1)\n","graph0 = ego_graphs[str(id0)]\n","graph1 = ego_graphs[str(id1)]\n","# verify that both graphs have the correct label\n","print('label of first graph:',labels.loc[id0,'target'])\n","print('label of second graph:',labels.loc[id1,'target'])\n","\n","# Visualize the graphs\n","fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n","plt.subplot(121)\n","nx.draw(graph0, with_labels=True, node_size=200, node_color='lightblue', edge_color='gray')\n","axes[0].set_title('Graph with label 0')\n","plt.subplot(122)\n","nx.draw(graph1, with_labels=True, node_size=200, node_color='lightblue', edge_color='gray')\n","axes[1].set_title('Graph with label 1')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1686304885877,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"F32JKOcNBy0T","outputId":"89676d1d-58c2-4013-b999-f5f2e4bd5f13"},"outputs":[],"source":["graph0.number_of_nodes()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":219772,"status":"ok","timestamp":1686305105645,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"D-83YzQxdOud"},"outputs":[],"source":["d = {'Label': [0,1], 'min nodes': [1e5, 1e5], 'max nodes': [0, 0], 'average nodes': [0, 0],\n","     'min edges': [1e5, 1e5], 'max edges': [0, 0], 'average edges': [0, 0],\n","     'min sparsity': [1e5, 1e5], 'max sparsity': [0, 0], 'average sparsity': [0, 0],\n","     'min diameter': [1e5, 1e5], 'max diameter': [0, 0], 'average diameter': [0, 0],\n","     'min density': [1e5, 1e5], 'max density': [0, 0], 'average density': [0, 0]}\n","stats = pd.DataFrame(data=d)\n","\n","\n","for id in range(len(labels)):\n","  lbl = labels.loc[id,'target']\n","  graph = ego_graphs[str(id)]\n","\n","  num_nodes = graph.number_of_nodes()\n","  num_edges = graph.number_of_edges()\n","  sparsity = num_edges / (num_nodes * (num_nodes - 1))\n","  diameter = nx.diameter(graph)\n","  density = nx.density(graph)\n","\n","  if num_nodes<stats.loc[lbl,'min nodes']:\n","    stats.loc[lbl,'min nodes'] = num_nodes\n","  elif num_nodes>stats.loc[lbl,'max nodes']:\n","    stats.loc[lbl,'max nodes'] = num_nodes\n","\n","  if num_edges<stats.loc[lbl,'min edges']:\n","    stats.loc[lbl,'min edges'] = num_edges\n","  elif num_edges>stats.loc[lbl,'max edges']:\n","    stats.loc[lbl,'max edges'] = num_edges\n","\n","  if sparsity<stats.loc[lbl,'min sparsity']:\n","    stats.loc[lbl,'min sparsity'] = sparsity\n","  elif sparsity>stats.loc[lbl,'max sparsity']:\n","    stats.loc[lbl,'max sparsity'] = sparsity\n","\n","  if diameter<stats.loc[lbl,'min diameter']:\n","    stats.loc[lbl,'min diameter'] = diameter\n","  elif diameter>stats.loc[lbl,'max diameter']:\n","    stats.loc[lbl,'max diameter'] = diameter\n","\n","  if density<stats.loc[lbl,'min density']:\n","    stats.loc[lbl,'min density'] = density\n","  elif density>stats.loc[lbl,'max density']:\n","    stats.loc[lbl,'max density'] = density\n","\n","  stats.loc[lbl,'average nodes'] += num_nodes\n","  stats.loc[lbl,'average edges'] += num_edges\n","  stats.loc[lbl,'average sparsity'] += sparsity\n","  stats.loc[lbl,'average diameter'] += diameter\n","  stats.loc[lbl,'average density'] += density\n","\n","stats.loc[0,'average nodes'] /= len(index0)\n","stats.loc[1,'average nodes'] /= len(index1)\n","stats.loc[0,'average edges'] /= len(index0)\n","stats.loc[1,'average edges'] /= len(index1)\n","stats.loc[0,'average sparsity'] /= len(index0)\n","stats.loc[1,'average sparsity'] /= len(index1)\n","stats.loc[0,'average diameter'] /= len(index0)\n","stats.loc[1,'average diameter'] /= len(index1)\n","stats.loc[0,'average density'] /= len(index0)\n","stats.loc[1,'average density'] /= len(index1)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":173},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1686305105645,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"lpkMHNCQ_cWT","outputId":"48ef0f54-22ac-4d49-b012-91d079a81937"},"outputs":[],"source":["stats"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tdLmBdl3Fznd"},"source":["### Extract Features"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1686305105646,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"OkdxI9Br9qCH"},"outputs":[],"source":["from networkx.algorithms.centrality import betweenness\n","def node_feature_extraction(G):\n","  '''\n","  INPUT:\n","  G: the graph\n","\n","  OUTPUT:\n","  features: feature matrix of dimensions (N, D) (N: number of samples; D: number of features) \n","  '''\n","  num_nodes = G.number_of_nodes()\n","  num_edges = G.number_of_edges()\n","  sparsity = num_edges / (num_nodes * (num_nodes - 1))\n","  diameter = nx.diameter(G)\n","  density = nx.density(G)\n","\n","  features = np.array([num_nodes, num_edges, sparsity, diameter, density])\n","\n","  degrees = G.degree()\n","  clustering = nx.clustering(G)\n","  betweenness = nx.betweenness_centrality(G)\n","  eigenvector = nx.eigenvector_centrality(G)\n","\n","  features = np.concatenate((features,np.array([[val for (node, val) in degrees], list(clustering.values()), list(betweenness.values()), list(eigenvector.values())]).T[0]), axis=None)\n","  return features"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":645967,"status":"ok","timestamp":1686305751600,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"3als80QpGH2S","outputId":"cb3c617b-ee9c-479f-ea02-2b7b75e30270"},"outputs":[],"source":["targets = np.array(labels['target'])\n","features = np.zeros((len(labels), 9))\n","for id in range(len(labels)):\n","  graph = ego_graphs[str(id)]\n","  features[id] = node_feature_extraction(graph)\n","\n","print(np.shape(targets))\n","print(np.shape(features))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pDKEvvUKI_5K"},"source":["### SVM"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1368,"status":"ok","timestamp":1686305752957,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"RJ26nokFJBjJ"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","from sklearn import metrics\n","from sklearn import svm\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.feature_selection import SelectKBest, f_classif\n","import seaborn as sns\n","\n","\n","def classifier(features, targets, feature_selection, num_features, test_size, seed=0, verbose=False):\n"," \n","  # Split the data into training and testing sets, with test_size=0.5\n","  X_train, X_test, y_train, y_test = train_test_split(features, targets, stratify=targets, test_size=test_size, \n","                                                      random_state=seed)\n","\n","  if feature_selection:\n","    ## Build and train the ML model, including feature selection, normalization and Support Vector Classifier. Select the k highest relevant features for the classification. \n","    selector = SelectKBest(f_classif, k=num_features)\n","    X_train_selected = selector.fit_transform(X_train, y_train)\n","    X_test_selected = selector.transform(X_test)\n","    ## Print the scores for individual features.\n","    ## Plot the feature scores\n","    plt.figure(figsize=(10, 6))\n","    plt.bar(range(len(selector.scores_)), selector.scores_)\n","    plt.xlabel('Feature Index')\n","    plt.ylabel('Score')\n","    plt.title('Feature Scores')\n","    plt.show()\n","\n","  else:\n","    ## Build and train the ML model, including normalization and Support Vector Classifier.\n","    X_train_selected = X_train\n","    X_test_selected = X_test\n","  scaler = StandardScaler()\n","  X_train_norm = scaler.fit_transform(X_train_selected)\n","  X_test_norm = scaler.transform(X_test_selected)\n","\n","  clf = svm.SVC(random_state=seed, class_weight='balanced')\n","  clf.fit(X_train_norm, y_train)\n","\n","  # Use the model to predict the labels of the test data\n","  y_pred = clf.predict(X_test_norm)\n","\n","  # Output the confusion matrix and weighted f1 score on the test set. Print the weighted f1 score and plot the confusion matrix if verbose\n","  cm = metrics.confusion_matrix(y_test, y_pred)\n","  f1 = metrics.f1_score(y_test, y_pred, average='weighted')\n","  if verbose:\n","      print('Weighted F1 score:', f1)\n","      # Plot the confusion matrix\n","      plt.figure(figsize=(8, 6))\n","      sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n","      plt.xlabel('Predicted Label')\n","      plt.ylabel('True Label')\n","      plt.title('Confusion Matrix')\n","      plt.show()\n","  return f1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":517452,"status":"ok","timestamp":1686307765850,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"1btmE9ByNsPF","outputId":"f72bd0ad-fedc-4482-e96b-f23144f0a347"},"outputs":[],"source":["classifier(features, targets, feature_selection=True, num_features=4, \n","           test_size=0.4, seed=0, verbose=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Dkdo-mGGdOud"},"source":["## Global Properties"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1686306341484,"user":{"displayName":"Jad Sobhie","userId":"06441007241686934360"},"user_tz":-120},"id":"_D7UX1aOdOud","outputId":"88055fb1-bf79-4033-e650-0fb7fa784cd0"},"outputs":[],"source":["#Connected components\n","connected_components = nx.connected_components(graph)\n","num_connected_components = len(list(connected_components))\n","print(f\"Connected components: {num_connected_components}\")\n","\n","#Sparsity\n","num_nodes = graph.number_of_nodes()\n","num_edges = graph.number_of_edges()\n","sparsity = num_edges / (num_nodes * (num_nodes - 1))\n","print(f\"Sparsity: {sparsity}\")\n","\n","#Diameter\n","diameter = nx.diameter(graph)\n","print(f\"Diameter: {diameter}\")\n","\n","#Clustering\n","clusters = nx.clustering(graph)\n","print(f\"Clusters: {clusters}\")\n","\n","#Degree distribution\n","degree_sequence = sorted([d for n, d in graph.degree()], reverse=True)\n","print(f\"Degree distribution: {degree_sequence}\")\n","\n","#Spectrum (eigenvalues of adjacency matrix)\n","eigenvalues = nx.linalg.spectrum.adjacency_spectrum(graph)\n","print(f\"Spectrum: {eigenvalues}\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# MLP Classifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import accuracy_score\n","\n","# Assuming you have the features matrix (X) and the target vector (y)\n","# X shape: (127094, #number of features) depends on what features you choose\n","# y shape: (127094,)\n","\n","# Splitting the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(features, targets, test_size=0.2, random_state=42)\n","\n","# Creating a neural network classifier\n","classifier = MLPClassifier(hidden_layer_sizes=(100, 100), max_iter=100, random_state=42)\n","\n","# Training the classifier\n","training_loss = classifier.fit(X_train, y_train).loss_curve_\n","testing_loss = classifier.fit(X_test, y_test).loss_curve_\n","\n","# Predicting on the training and testing sets\n","y_train_pred = classifier.predict(X_train)\n","y_test_pred = classifier.predict(X_test)\n","\n","# Calculating accuracy scores\n","train_accuracy = accuracy_score(y_train, y_train_pred)\n","test_accuracy = accuracy_score(y_test, y_test_pred)\n","\n","print(\"Training accuracy:\", train_accuracy)\n","print(\"Testing accuracy:\", test_accuracy)\n","\n","# Plotting the training and testing curves\n","plt.figure(figsize=(10, 6))\n","plt.plot(training_loss, label=\"Training Curve\")\n","plt.plot(testing_loss, label=\"Testing Curve\")\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss/Cross-Validation Score\")\n","plt.legend()\n","plt.title(\"Training and Testing Curves\")\n","plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Neural Network"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import tensorflow as tf\n","\n","# Step 1: Splitting the Data\n","X = features\n","y = targets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Step 2: Setting up the GCN Model\n","num_features = X_train.shape[1]\n","num_classes = 2  # Binary classification\n","\n","model = tf.keras.Sequential([\n","    tf.keras.layers.Dense(64, activation='relu', input_shape=(num_features,)),\n","    tf.keras.layers.Dense(num_classes, activation='softmax')\n","])\n","\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Step 3: Training the Model\n","num_epochs = 10\n","\n","model.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_data=(X_test, y_test))\n","\n","history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=32, validation_data=(X_test, y_test))\n","\n","# Get the training and testing accuracy values from the history\n","train_acc = history.history['accuracy']\n","val_acc = history.history['val_accuracy']\n","\n","# Generate x-axis values for the accuracy curves\n","epochs = range(1, num_epochs + 1)\n","\n","# Plot the training and testing accuracy curves\n","plt.plot(epochs, train_acc, 'b', label='Training Accuracy')\n","plt.plot(epochs, val_acc, 'r', label='Testing Accuracy')\n","plt.title('Training and Testing Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
